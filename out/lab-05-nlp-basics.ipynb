{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: NLP Basics\n",
        "\n",
        "**Day 3 - From Deep Learning to LLMs**\n",
        "\n",
        "| Duration | Difficulty | Prerequisites |\n",
        "|----------|------------|---------------|\n",
        "| 75 min | Intermediate | Labs 1-4 |\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand text preprocessing and tokenization\n",
        "- Learn about word embeddings\n",
        "- Implement basic attention mechanism\n",
        "- Understand transformer architecture concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1: Text Preprocessing\n",
        "\n",
        "Before feeding text to models, we need to clean and normalize it.\n",
        "\n",
        "**Your Task:** Implement basic text preprocessing functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean text by:\n",
        "    1. Converting to lowercase\n",
        "    2. Removing special characters (keep only letters, numbers, spaces)\n",
        "    3. Removing extra whitespace\n",
        "    \"\"\"\n",
        "    # TODO: Convert to lowercase\n",
        "    text = None\n",
        "    \n",
        "    # TODO: Remove special characters (use re.sub)\n",
        "    # Pattern: [^a-z0-9\\s] matches anything that's not letter, number, or space\n",
        "    text = None\n",
        "    \n",
        "    # TODO: Remove extra whitespace\n",
        "    text = None\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_simple(text):\n",
        "    \"\"\"\n",
        "    Simple word-level tokenization.\n",
        "    Split text into words.\n",
        "    \"\"\"\n",
        "    # TODO: Split text on whitespace\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocabulary(texts, min_freq=1):\n",
        "    \"\"\"\n",
        "    Build a vocabulary from list of texts.\n",
        "    \n",
        "    Returns:\n",
        "        word2idx: Dictionary mapping words to indices\n",
        "        idx2word: Dictionary mapping indices to words\n",
        "    \"\"\"\n",
        "    # TODO: Count all words across all texts\n",
        "    word_counts = Counter()\n",
        "    # for text in texts:\n",
        "    #     tokens = tokenize_simple(clean_text(text))\n",
        "    #     word_counts.update(tokens)\n",
        "    \n",
        "    # TODO: Filter by minimum frequency\n",
        "    # vocab = [word for word, count in word_counts.items() if count >= min_freq]\n",
        "    \n",
        "    # TODO: Create word2idx with special tokens\n",
        "    # Start with: {'<PAD>': 0, '<UNK>': 1}\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    # for word in vocab:\n",
        "    #     word2idx[word] = len(word2idx)\n",
        "    \n",
        "    # TODO: Create idx2word (reverse mapping)\n",
        "    idx2word = {v: k for k, v in word2idx.items()}\n",
        "    \n",
        "    return word2idx, idx2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 1\n",
        "sample_texts = [\n",
        "    \"Hello, World! This is a TEST.\",\n",
        "    \"Machine Learning is AMAZING!!!\",\n",
        "    \"Neural networks learn from data.\"\n",
        "]\n",
        "\n",
        "# Test clean_text\n",
        "cleaned = clean_text(sample_texts[0])\n",
        "print(f\"Original: {sample_texts[0]}\")\n",
        "print(f\"Cleaned: {cleaned}\")\n",
        "\n",
        "# Test tokenize\n",
        "if cleaned:\n",
        "    tokens = tokenize_simple(cleaned)\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "\n",
        "# Test vocabulary\n",
        "word2idx, idx2word = build_vocabulary(sample_texts)\n",
        "print(f\"\\nVocabulary size: {len(word2idx)}\")\n",
        "print(f\"Sample: {dict(list(word2idx.items())[:5])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 2: Text to Sequences\n",
        "\n",
        "Convert text to numerical sequences for model input.\n",
        "\n",
        "**Your Task:** Implement text encoding/decoding functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_text(text, word2idx, max_length=None):\n",
        "    \"\"\"\n",
        "    Convert text to sequence of indices.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text\n",
        "        word2idx: Vocabulary mapping\n",
        "        max_length: If provided, pad or truncate to this length\n",
        "    \n",
        "    Returns:\n",
        "        List of token indices\n",
        "    \"\"\"\n",
        "    # TODO: Clean and tokenize text\n",
        "    tokens = None\n",
        "    \n",
        "    # TODO: Convert tokens to indices (use UNK for unknown words)\n",
        "    # indices = [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
        "    indices = None\n",
        "    \n",
        "    # TODO: Handle max_length (pad with 0 or truncate)\n",
        "    if max_length:\n",
        "        pass\n",
        "    \n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_sequence(indices, idx2word):\n",
        "    \"\"\"\n",
        "    Convert sequence of indices back to text.\n",
        "    \n",
        "    Skip padding tokens (index 0).\n",
        "    \"\"\"\n",
        "    # TODO: Convert indices to words, skipping PAD\n",
        "    # words = [idx2word[idx] for idx in indices if idx != 0]\n",
        "    # return ' '.join(words)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 2\n",
        "test_text = \"Neural networks are powerful models.\"\n",
        "\n",
        "encoded = encode_text(test_text, word2idx, max_length=10)\n",
        "print(f\"Text: {test_text}\")\n",
        "print(f\"Encoded: {encoded}\")\n",
        "\n",
        "if encoded:\n",
        "    decoded = decode_sequence(encoded, idx2word)\n",
        "    print(f\"Decoded: {decoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 3: Word Embeddings\n",
        "\n",
        "Embeddings convert sparse indices to dense vectors.\n",
        "\n",
        "**Your Task:** Understand and use embedding layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleEmbedding:\n",
        "    \"\"\"\n",
        "    Manual implementation of word embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        \"\"\"\n",
        "        Initialize embedding matrix with random values.\n",
        "        \n",
        "        Shape: (vocab_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        # TODO: Initialize embedding matrix\n",
        "        # Use small random values: np.random.randn(...) * 0.1\n",
        "        self.embeddings = None\n",
        "    \n",
        "    def __call__(self, indices):\n",
        "        \"\"\"\n",
        "        Look up embeddings for given indices.\n",
        "        \n",
        "        Args:\n",
        "            indices: List or array of word indices\n",
        "        \n",
        "        Returns:\n",
        "            Array of shape (len(indices), embedding_dim)\n",
        "        \"\"\"\n",
        "        # TODO: Return embeddings for the given indices\n",
        "        # Hint: self.embeddings[indices]\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two vectors.\n",
        "    \n",
        "    Formula: cos(a,b) = (a Â· b) / (||a|| * ||b||)\n",
        "    \"\"\"\n",
        "    # TODO: Implement cosine similarity\n",
        "    # dot_product = np.dot(vec1, vec2)\n",
        "    # norm1 = np.linalg.norm(vec1)\n",
        "    # norm2 = np.linalg.norm(vec2)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 3\n",
        "vocab_size = len(word2idx)\n",
        "embedding_dim = 50\n",
        "\n",
        "# Manual embedding\n",
        "simple_emb = SimpleEmbedding(vocab_size, embedding_dim)\n",
        "\n",
        "if simple_emb.embeddings is not None:\n",
        "    print(f\"Embedding matrix shape: {simple_emb.embeddings.shape}\")\n",
        "    \n",
        "    # Look up embeddings\n",
        "    test_indices = [2, 3, 4]\n",
        "    embeddings = simple_emb(test_indices)\n",
        "    print(f\"Embeddings for indices {test_indices}: shape {embeddings.shape}\")\n",
        "\n",
        "# PyTorch embedding layer\n",
        "torch_emb = nn.Embedding(vocab_size, embedding_dim)\n",
        "print(f\"\\nPyTorch Embedding: {torch_emb}\")\n",
        "print(f\"Parameters: {torch_emb.weight.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 4: Self-Attention Mechanism\n",
        "\n",
        "Attention allows models to focus on relevant parts of the input.\n",
        "\n",
        "**Your Task:** Implement scaled dot-product attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention.\n",
        "    \n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "    \n",
        "    Args:\n",
        "        query: Shape (seq_len, d_k)\n",
        "        key: Shape (seq_len, d_k)\n",
        "        value: Shape (seq_len, d_v)\n",
        "    \n",
        "    Returns:\n",
        "        output: Shape (seq_len, d_v)\n",
        "        attention_weights: Shape (seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    d_k = query.shape[-1]\n",
        "    \n",
        "    # TODO: Compute attention scores: Q @ K^T\n",
        "    scores = None\n",
        "    \n",
        "    # TODO: Scale by sqrt(d_k)\n",
        "    scores = None\n",
        "    \n",
        "    # TODO: Apply softmax to get attention weights\n",
        "    attention_weights = None\n",
        "    \n",
        "    # TODO: Compute output: weights @ V\n",
        "    output = None\n",
        "    \n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_attention(attention_weights, tokens):\n",
        "    \"\"\"\n",
        "    Visualize attention weights as a heatmap.\n",
        "    \"\"\"\n",
        "    # TODO: Use plt.imshow to create heatmap\n",
        "    # Add token labels on axes\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 4\n",
        "# Simulate embeddings for a sentence\n",
        "seq_len = 5\n",
        "d_model = 8\n",
        "\n",
        "# Random embeddings (in practice, these would be learned)\n",
        "embeddings = torch.randn(seq_len, d_model)\n",
        "\n",
        "# For self-attention, Q=K=V=embeddings\n",
        "output, weights = scaled_dot_product_attention(embeddings, embeddings, embeddings)\n",
        "\n",
        "if output is not None:\n",
        "    print(f\"Input shape: {embeddings.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Attention weights shape: {weights.shape}\")\n",
        "    print(f\"\\nAttention weights (each row sums to 1):\")\n",
        "    print(weights.numpy().round(3))\n",
        "    \n",
        "    # Visualize\n",
        "    tokens = ['The', 'cat', 'sat', 'on', 'mat']\n",
        "    visualize_attention(weights.numpy(), tokens)\n",
        "else:\n",
        "    print(\"Implement scaled_dot_product_attention()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 5: Simple Transformer Block\n",
        "\n",
        "**Your Task:** Build a basic transformer encoder block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-attention layer using PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        # TODO: Create linear layers for Q, K, V projections\n",
        "        # self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        # self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        # self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # TODO: Project inputs to Q, K, V\n",
        "        # Q = self.query(x)\n",
        "        # K = self.key(x)\n",
        "        # V = self.value(x)\n",
        "        \n",
        "        # TODO: Compute attention\n",
        "        # Use F.scaled_dot_product_attention or implement manually\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic transformer encoder block.\n",
        "    \n",
        "    Structure:\n",
        "    x -> Self-Attention -> Add & Norm -> FFN -> Add & Norm -> output\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super().__init__()\n",
        "        # TODO: Self-attention\n",
        "        # self.attention = SelfAttention(embed_dim)\n",
        "        \n",
        "        # TODO: Feed-forward network\n",
        "        # self.ffn = nn.Sequential(\n",
        "        #     nn.Linear(embed_dim, ff_dim),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(ff_dim, embed_dim)\n",
        "        # )\n",
        "        \n",
        "        # TODO: Layer normalization\n",
        "        # self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        # self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Attention with residual connection\n",
        "        # attn_out = self.attention(x)\n",
        "        # x = self.norm1(x + attn_out)\n",
        "        \n",
        "        # TODO: FFN with residual connection\n",
        "        # ffn_out = self.ffn(x)\n",
        "        # x = self.norm2(x + ffn_out)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 5\n",
        "embed_dim = 64\n",
        "ff_dim = 128\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "# Create transformer block\n",
        "transformer = TransformerBlock(embed_dim, ff_dim)\n",
        "\n",
        "if hasattr(transformer, 'attention'):\n",
        "    print(\"TransformerBlock:\")\n",
        "    print(transformer)\n",
        "    \n",
        "    # Test forward pass\n",
        "    test_input = torch.randn(batch_size, seq_len, embed_dim)\n",
        "    output = transformer(test_input)\n",
        "    print(f\"\\nInput shape: {test_input.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "else:\n",
        "    print(\"Implement TransformerBlock\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 6: Text Classification with Attention\n",
        "\n",
        "**Your Task:** Build a simple text classifier using attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple text classifier:\n",
        "    Embedding -> Self-Attention -> Global Average Pooling -> Classification\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super().__init__()\n",
        "        # TODO: Embedding layer\n",
        "        # TODO: Attention layer\n",
        "        # TODO: Classification head\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Token indices of shape (batch, seq_len)\n",
        "        \"\"\"\n",
        "        # TODO: Embed tokens\n",
        "        # TODO: Apply attention\n",
        "        # TODO: Global average pool over sequence\n",
        "        # TODO: Classify\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 6\n",
        "vocab_size = 1000\n",
        "embed_dim = 32\n",
        "num_classes = 2\n",
        "\n",
        "classifier = TextClassifier(vocab_size, embed_dim, num_classes)\n",
        "\n",
        "if hasattr(classifier, 'embedding'):\n",
        "    print(\"TextClassifier:\")\n",
        "    print(classifier)\n",
        "    \n",
        "    # Test\n",
        "    batch = torch.randint(0, vocab_size, (4, 20))  # 4 samples, 20 tokens\n",
        "    output = classifier(batch)\n",
        "    print(f\"\\nInput shape: {batch.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "else:\n",
        "    print(\"Implement TextClassifier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Checkpoint\n",
        "\n",
        "Congratulations! You've completed Lab 5.\n",
        "\n",
        "### Key Takeaways:\n",
        "- Tokenization converts text to sequences\n",
        "- Embeddings map tokens to dense vectors\n",
        "- Attention allows focusing on relevant parts\n",
        "- Transformers use attention + feed-forward layers\n",
        "\n",
        "**Next:** Lab 6 - LLM APIs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
