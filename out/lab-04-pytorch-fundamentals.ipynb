{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4: PyTorch Fundamentals\n",
        "\n",
        "**Day 2 - Deep Learning**\n",
        "\n",
        "| Duration | Difficulty | Prerequisites |\n",
        "|----------|------------|---------------|\n",
        "| 90 min | Intermediate | Lab 3 |\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Work with PyTorch tensors\n",
        "- Understand automatic differentiation (autograd)\n",
        "- Build neural networks with nn.Module\n",
        "- Train a model with backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1: Tensor Basics\n",
        "\n",
        "Tensors are the fundamental data structure in PyTorch (like NumPy arrays, but GPU-capable).\n",
        "\n",
        "**Your Task:** Create and manipulate tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_tensors():\n",
        "    \"\"\"Create various tensors.\"\"\"\n",
        "    # TODO: Create a tensor from a Python list [1, 2, 3, 4]\n",
        "    tensor_from_list = None\n",
        "    \n",
        "    # TODO: Create a 3x3 tensor of zeros\n",
        "    zeros = None\n",
        "    \n",
        "    # TODO: Create a 3x3 tensor of ones\n",
        "    ones = None\n",
        "    \n",
        "    # TODO: Create a 3x3 tensor with random values (uniform 0-1)\n",
        "    random_tensor = None\n",
        "    \n",
        "    # TODO: Create a tensor from numpy array\n",
        "    np_array = np.array([[1, 2], [3, 4]])\n",
        "    from_numpy = None\n",
        "    \n",
        "    return tensor_from_list, zeros, ones, random_tensor, from_numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tensor_operations():\n",
        "    \"\"\"Perform tensor operations.\"\"\"\n",
        "    a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "    b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
        "    \n",
        "    # TODO: Element-wise addition\n",
        "    add_result = None\n",
        "    \n",
        "    # TODO: Element-wise multiplication\n",
        "    mul_result = None\n",
        "    \n",
        "    # TODO: Matrix multiplication (use @ or torch.matmul)\n",
        "    matmul_result = None\n",
        "    \n",
        "    # TODO: Sum all elements\n",
        "    sum_result = None\n",
        "    \n",
        "    # TODO: Mean of all elements\n",
        "    mean_result = None\n",
        "    \n",
        "    return add_result, mul_result, matmul_result, sum_result, mean_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 1\n",
        "tensors = create_tensors()\n",
        "print(\"Created tensors:\")\n",
        "for i, t in enumerate(tensors):\n",
        "    if t is not None:\n",
        "        print(f\"  Tensor {i+1}: shape={t.shape}, dtype={t.dtype}\")\n",
        "\n",
        "ops = tensor_operations()\n",
        "print(\"\\nTensor operations:\")\n",
        "names = ['Add', 'Mul', 'MatMul', 'Sum', 'Mean']\n",
        "for name, result in zip(names, ops):\n",
        "    if result is not None:\n",
        "        print(f\"  {name}: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 2: Automatic Differentiation\n",
        "\n",
        "PyTorch's autograd automatically computes gradients.\n",
        "\n",
        "**Your Task:** Use autograd to compute derivatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradients():\n",
        "    \"\"\"\n",
        "    Compute gradients using autograd.\n",
        "    \n",
        "    Given y = x^2 + 3x + 1, compute dy/dx at x=2\n",
        "    Expected: dy/dx = 2x + 3 = 2(2) + 3 = 7\n",
        "    \"\"\"\n",
        "    # TODO: Create tensor x=2 with requires_grad=True\n",
        "    x = None\n",
        "    \n",
        "    # TODO: Compute y = x^2 + 3x + 1\n",
        "    y = None\n",
        "    \n",
        "    # TODO: Compute gradient (backward pass)\n",
        "    # y.backward()\n",
        "    \n",
        "    # TODO: Return the gradient (x.grad)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chain_rule_example():\n",
        "    \"\"\"\n",
        "    Demonstrate chain rule: if y = f(g(x)), then dy/dx = df/dg * dg/dx\n",
        "    \n",
        "    Let g(x) = 2x, f(g) = g^2\n",
        "    Then y = (2x)^2 = 4x^2\n",
        "    dy/dx = 8x, so at x=3, dy/dx = 24\n",
        "    \"\"\"\n",
        "    # TODO: Create x=3 with gradient tracking\n",
        "    x = None\n",
        "    \n",
        "    # TODO: Compute g = 2x\n",
        "    g = None\n",
        "    \n",
        "    # TODO: Compute y = g^2\n",
        "    y = None\n",
        "    \n",
        "    # TODO: Backward pass and return gradient\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 2\n",
        "grad1 = compute_gradients()\n",
        "print(f\"Gradient of x^2 + 3x + 1 at x=2: {grad1} (expected: 7)\")\n",
        "\n",
        "grad2 = chain_rule_example()\n",
        "print(f\"Gradient of (2x)^2 at x=3: {grad2} (expected: 24)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 3: Building Neural Networks with nn.Module\n",
        "\n",
        "**Your Task:** Create a neural network using PyTorch's nn.Module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple neural network: 2 inputs -> 4 hidden -> 1 output\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: Define layers\n",
        "        # self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        # self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        # self.activation = nn.ReLU()\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: x -> layer1 -> relu -> layer2 -> output\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FlexibleNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Flexible network with configurable architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_sizes):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            layer_sizes: List like [input, hidden1, hidden2, output]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # TODO: Create layers dynamically\n",
        "        # Hint: Use nn.ModuleList to store layers\n",
        "        # self.layers = nn.ModuleList()\n",
        "        # for i in range(len(layer_sizes) - 1):\n",
        "        #     self.layers.append(nn.Linear(...))\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Pass through all layers with ReLU\n",
        "        # Apply ReLU to all except the last layer\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 3\n",
        "simple_net = SimpleNet()\n",
        "print(\"SimpleNet architecture:\")\n",
        "print(simple_net)\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "if hasattr(simple_net, 'layer1'):\n",
        "    print(f\"Total parameters: {count_parameters(simple_net)}\")\n",
        "    \n",
        "    # Test forward pass\n",
        "    test_input = torch.randn(5, 2)\n",
        "    output = simple_net(test_input)\n",
        "    print(f\"Input shape: {test_input.shape}, Output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 4: Training Loop\n",
        "\n",
        "**Your Task:** Train a network to learn the XOR function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XOR dataset\n",
        "X_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "\n",
        "print(\"XOR Truth Table:\")\n",
        "for x, y in zip(X_xor, y_xor):\n",
        "    print(f\"  {x.tolist()} -> {y.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class XORNet(nn.Module):\n",
        "    \"\"\"Network to solve XOR problem.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # XOR needs a hidden layer!\n",
        "        # TODO: Define layers\n",
        "        # 2 inputs -> 4 hidden (with ReLU) -> 1 output (with Sigmoid)\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_xor_network(model, X, y, epochs=1000, lr=0.1):\n",
        "    \"\"\"\n",
        "    Train the XOR network.\n",
        "    \n",
        "    Steps in each epoch:\n",
        "    1. Forward pass: predictions = model(X)\n",
        "    2. Compute loss: loss = criterion(predictions, y)\n",
        "    3. Backward pass: loss.backward()\n",
        "    4. Update weights: optimizer.step()\n",
        "    5. Zero gradients: optimizer.zero_grad()\n",
        "    \"\"\"\n",
        "    # TODO: Define loss function (BCELoss for binary classification)\n",
        "    criterion = None\n",
        "    \n",
        "    # TODO: Define optimizer (Adam or SGD)\n",
        "    optimizer = None\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # TODO: Implement training step\n",
        "        # 1. Forward pass\n",
        "        # 2. Compute loss\n",
        "        # 3. Zero gradients\n",
        "        # 4. Backward pass\n",
        "        # 5. Update weights\n",
        "        \n",
        "        pass\n",
        "        \n",
        "        # Record loss (uncomment when implemented)\n",
        "        # losses.append(loss.item())\n",
        "    \n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 4\n",
        "xor_model = XORNet()\n",
        "\n",
        "if hasattr(xor_model, 'layer1'):\n",
        "    print(\"Before training:\")\n",
        "    with torch.no_grad():\n",
        "        predictions = xor_model(X_xor)\n",
        "        for x, pred, actual in zip(X_xor, predictions, y_xor):\n",
        "            print(f\"  {x.tolist()} -> {pred.item():.4f} (expected {actual.item()})\")\n",
        "    \n",
        "    # Train\n",
        "    losses = train_xor_network(xor_model, X_xor, y_xor, epochs=2000, lr=0.5)\n",
        "    \n",
        "    if losses:\n",
        "        print(\"\\nAfter training:\")\n",
        "        with torch.no_grad():\n",
        "            predictions = xor_model(X_xor)\n",
        "            for x, pred, actual in zip(X_xor, predictions, y_xor):\n",
        "                print(f\"  {x.tolist()} -> {pred.item():.4f} (expected {actual.item()})\")\n",
        "        \n",
        "        # Plot loss\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('XOR Training Loss')\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Implement XORNet class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 5: Using nn.Sequential\n",
        "\n",
        "**Your Task:** Build networks quickly using nn.Sequential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequential_network():\n",
        "    \"\"\"\n",
        "    Create a network using nn.Sequential.\n",
        "    \n",
        "    Architecture: 10 -> 64 (ReLU) -> 32 (ReLU) -> 1 (Sigmoid)\n",
        "    \"\"\"\n",
        "    # TODO: Use nn.Sequential to define the network\n",
        "    # model = nn.Sequential(\n",
        "    #     nn.Linear(...),\n",
        "    #     nn.ReLU(),\n",
        "    #     ...\n",
        "    # )\n",
        "    model = None\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 5\n",
        "seq_model = create_sequential_network()\n",
        "\n",
        "if seq_model is not None:\n",
        "    print(\"Sequential model:\")\n",
        "    print(seq_model)\n",
        "    print(f\"\\nTotal parameters: {count_parameters(seq_model)}\")\n",
        "    \n",
        "    # Test\n",
        "    test_input = torch.randn(5, 10)\n",
        "    output = seq_model(test_input)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "else:\n",
        "    print(\"Implement create_sequential_network()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 6: Complete Training Pipeline\n",
        "\n",
        "**Your Task:** Train a classifier on synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate data\n",
        "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', alpha=0.6)\n",
        "plt.title('Two Moons Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_classifier(X_train, y_train, X_test, y_test, epochs=200):\n",
        "    \"\"\"\n",
        "    Complete training pipeline for binary classification.\n",
        "    \n",
        "    Returns:\n",
        "        model: Trained model\n",
        "        train_losses: List of training losses\n",
        "        test_accuracies: List of test accuracies\n",
        "    \"\"\"\n",
        "    # TODO: Create model (2 -> 16 -> 8 -> 1 with sigmoid output)\n",
        "    model = None\n",
        "    \n",
        "    # TODO: Define loss and optimizer\n",
        "    criterion = None\n",
        "    optimizer = None\n",
        "    \n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # TODO: Training step\n",
        "        pass\n",
        "        \n",
        "        # TODO: Evaluate on test set (every 10 epochs)\n",
        "        # Calculate accuracy = (predictions > 0.5) == y_test\n",
        "    \n",
        "    return model, train_losses, test_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 6\n",
        "result = train_classifier(X_train_t, y_train_t, X_test_t, y_test_t, epochs=500)\n",
        "\n",
        "if result[0] is not None:\n",
        "    model, losses, accs = result\n",
        "    \n",
        "    # Plot results\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    axes[0].plot(losses)\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training Loss')\n",
        "    \n",
        "    axes[1].plot(range(0, 500, 10), accs)\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_title('Test Accuracy')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Final test accuracy: {accs[-1]:.2%}\")\n",
        "else:\n",
        "    print(\"Implement train_classifier()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Checkpoint\n",
        "\n",
        "Congratulations! You've completed Lab 4.\n",
        "\n",
        "### Key Takeaways:\n",
        "- Tensors are GPU-ready arrays with autograd support\n",
        "- nn.Module is the base class for all models\n",
        "- Training loop: forward -> loss -> backward -> update\n",
        "- nn.Sequential for quick model building\n",
        "\n",
        "**Next:** Lab 5 - NLP Basics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
