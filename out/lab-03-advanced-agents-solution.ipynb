{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Advanced Agent Patterns - SOLUTIONS\n",
    "\n",
    "**Module 3 - Advanced Agent Development**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|----------|------------|-----------|----------|\n",
    "| 120 min | Advanced | LangChain | 3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Chain-of-Thought Prompting - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cot_prompt(question: str) -> str:\n",
    "    \"\"\"Create a Chain-of-Thought prompt.\"\"\"\n",
    "    prompt = f\"\"\"You are a helpful assistant that solves problems step by step.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Let's solve this step by step:\n",
    "\n",
    "Step 1: First, let me identify what we need to find.\n",
    "\n",
    "Step 2: Now, let me break down the information given.\n",
    "\n",
    "Step 3: Let me work through the calculations or reasoning.\n",
    "\n",
    "Step 4: Finally, let me combine everything for the answer.\n",
    "\n",
    "Please show your complete reasoning for each step, then provide the final answer.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def solve_with_cot(question: str) -> dict:\n",
    "    \"\"\"Solve a problem using Chain-of-Thought.\"\"\"\n",
    "    prompt = create_cot_prompt(question)\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"reasoning\": response.content,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test problems\n",
    "problems = [\n",
    "    \"A store has 45 apples. They sell 12 in the morning and receive a shipment of 30. How many apples do they have now?\",\n",
    "    \"If a train travels at 60 mph for 2 hours, then 80 mph for 1.5 hours, what's the total distance?\",\n",
    "    \"A rectangle has a perimeter of 24 cm. If the length is twice the width, what are the dimensions?\"\n",
    "]\n",
    "\n",
    "for problem in problems:\n",
    "    result = solve_with_cot(problem)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Problem: {problem}\")\n",
    "    print(f\"\\nReasoning:\\n{result['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Plan-and-Execute Agent - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanAndExecuteAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.plan = []\n",
    "        self.results = []\n",
    "    \n",
    "    def create_plan(self, task: str) -> list:\n",
    "        \"\"\"Create a step-by-step plan for the task.\"\"\"\n",
    "        planning_prompt = f\"\"\"\n",
    "You are a planning assistant. Create a detailed step-by-step plan for this task.\n",
    "Return ONLY a numbered list (1., 2., 3., etc.) with 3-6 clear, actionable steps.\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Plan:\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke([HumanMessage(content=planning_prompt)])\n",
    "        \n",
    "        # Parse numbered list into steps\n",
    "        lines = response.content.strip().split('\\n')\n",
    "        self.plan = []\n",
    "        for line in lines:\n",
    "            # Match lines starting with numbers\n",
    "            line = line.strip()\n",
    "            if line and (line[0].isdigit() or line.startswith('-')):\n",
    "                # Remove numbering\n",
    "                clean_line = re.sub(r'^[\\d]+[.\\)]\\s*', '', line)\n",
    "                clean_line = re.sub(r'^-\\s*', '', clean_line)\n",
    "                if clean_line:\n",
    "                    self.plan.append(clean_line)\n",
    "        \n",
    "        return self.plan\n",
    "    \n",
    "    def execute_step(self, step: str, context: str) -> str:\n",
    "        \"\"\"Execute a single step of the plan.\"\"\"\n",
    "        execution_prompt = f\"\"\"\n",
    "You are executing a step in a larger plan.\n",
    "\n",
    "Previous context and results:\n",
    "{context if context else \"(This is the first step)\"}\n",
    "\n",
    "Current step to execute: {step}\n",
    "\n",
    "Execute this step thoroughly and provide the result:\n",
    "\"\"\"\n",
    "        response = self.llm.invoke([HumanMessage(content=execution_prompt)])\n",
    "        return response.content\n",
    "    \n",
    "    def run(self, task: str) -> dict:\n",
    "        \"\"\"Run the full plan-and-execute cycle.\"\"\"\n",
    "        # Reset state\n",
    "        self.plan = []\n",
    "        self.results = []\n",
    "        \n",
    "        # Phase 1: Planning\n",
    "        print(\"Creating plan...\")\n",
    "        plan = self.create_plan(task)\n",
    "        print(f\"Plan created with {len(plan)} steps:\")\n",
    "        for i, step in enumerate(plan):\n",
    "            print(f\"  {i+1}. {step}\")\n",
    "        \n",
    "        # Phase 2: Execution\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EXECUTION PHASE\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        context = \"\"\n",
    "        for i, step in enumerate(plan):\n",
    "            print(f\"\\n--- Executing Step {i+1} ---\")\n",
    "            print(f\"Step: {step}\")\n",
    "            result = self.execute_step(step, context)\n",
    "            self.results.append({\"step\": step, \"result\": result})\n",
    "            context += f\"\\n\\nStep {i+1} ({step}):\\n{result}\"\n",
    "            print(f\"Result: {result[:200]}...\" if len(result) > 200 else f\"Result: {result}\")\n",
    "        \n",
    "        return {\n",
    "            \"task\": task,\n",
    "            \"plan\": plan,\n",
    "            \"results\": self.results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "agent = PlanAndExecuteAgent(llm)\n",
    "result = agent.run(\"Research and summarize the key differences between BERT and GPT models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Self-Reflecting Agent - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveAgent:\n",
    "    def __init__(self, llm, max_iterations=3):\n",
    "        self.llm = llm\n",
    "        self.max_iterations = max_iterations\n",
    "    \n",
    "    def generate(self, task: str) -> str:\n",
    "        \"\"\"Generate initial response.\"\"\"\n",
    "        prompt = f\"Complete this task thoroughly:\\n\\n{task}\"\n",
    "        response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "        return response.content\n",
    "    \n",
    "    def critique(self, task: str, response: str) -> dict:\n",
    "        \"\"\"Critique the response and suggest improvements.\"\"\"\n",
    "        critique_prompt = f\"\"\"\n",
    "You are a critical reviewer. Analyze this response to a task.\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Response to critique:\n",
    "{response}\n",
    "\n",
    "Provide a structured critique with:\n",
    "1. Score (1-10, where 10 is perfect)\n",
    "2. Strengths (list 2-3)\n",
    "3. Weaknesses (list 2-3)\n",
    "4. Specific improvements needed (list 2-3 actionable items)\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{{\"score\": 7, \"strengths\": [\"str1\", \"str2\"], \"weaknesses\": [\"weak1\", \"weak2\"], \"improvements\": [\"imp1\", \"imp2\"]}}\n",
    "\"\"\"\n",
    "        result = self.llm.invoke([HumanMessage(content=critique_prompt)])\n",
    "        \n",
    "        try:\n",
    "            # Try to extract JSON from the response\n",
    "            content = result.content\n",
    "            # Find JSON in the response\n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group())\n",
    "            return json.loads(content)\n",
    "        except:\n",
    "            return {\"score\": 5, \"strengths\": [], \"weaknesses\": [\"Parse error\"], \"improvements\": [\"Could not parse critique\"]}\n",
    "    \n",
    "    def improve(self, task: str, response: str, critique: dict) -> str:\n",
    "        \"\"\"Improve response based on critique.\"\"\"\n",
    "        improve_prompt = f\"\"\"\n",
    "You previously wrote a response that received feedback. Now write an improved version.\n",
    "\n",
    "Original Task: {task}\n",
    "\n",
    "Your Previous Response:\n",
    "{response}\n",
    "\n",
    "Feedback Received:\n",
    "- Score: {critique.get('score', 'N/A')}/10\n",
    "- Weaknesses identified: {critique.get('weaknesses', [])}\n",
    "- Improvements needed: {critique.get('improvements', [])}\n",
    "\n",
    "Write a significantly improved response that addresses ALL the feedback:\n",
    "\"\"\"\n",
    "        result = self.llm.invoke([HumanMessage(content=improve_prompt)])\n",
    "        return result.content\n",
    "    \n",
    "    def run(self, task: str) -> dict:\n",
    "        \"\"\"Run the reflection loop.\"\"\"\n",
    "        print(f\"Task: {task}\\n\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"ITERATION 0: Initial Generation\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        response = self.generate(task)\n",
    "        print(f\"Response: {response[:300]}...\\n\" if len(response) > 300 else f\"Response: {response}\\n\")\n",
    "        \n",
    "        history = [{\"iteration\": 0, \"response\": response}]\n",
    "        \n",
    "        for i in range(self.max_iterations):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"ITERATION {i+1}: Critique & Improve\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            critique = self.critique(task, response)\n",
    "            print(f\"Score: {critique.get('score', 'N/A')}/10\")\n",
    "            print(f\"Strengths: {critique.get('strengths', [])}\")\n",
    "            print(f\"Weaknesses: {critique.get('weaknesses', [])}\")\n",
    "            print(f\"Improvements: {critique.get('improvements', [])}\")\n",
    "            \n",
    "            if critique.get('score', 0) >= 8:\n",
    "                print(\"\\nQuality threshold met! Stopping iterations.\")\n",
    "                break\n",
    "            \n",
    "            print(\"\\nGenerating improved response...\")\n",
    "            response = self.improve(task, response, critique)\n",
    "            print(f\"Improved: {response[:300]}...\" if len(response) > 300 else f\"Improved: {response}\")\n",
    "            \n",
    "            history.append({\n",
    "                \"iteration\": i + 1,\n",
    "                \"critique\": critique,\n",
    "                \"response\": response\n",
    "            })\n",
    "        \n",
    "        return {\"final_response\": response, \"history\": history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the reflective agent\n",
    "agent = ReflectiveAgent(llm, max_iterations=3)\n",
    "result = agent.run(\"Write a concise explanation of how neural networks learn\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*50)\n",
    "print(result['final_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "Congratulations! You've completed Lab 3 with solutions. Key takeaways:\n",
    "\n",
    "- **Chain-of-Thought**: Explicit step prompts improve reasoning\n",
    "- **Plan-and-Execute**: Separating planning from execution gives better control\n",
    "- **Self-Reflection**: Critique loops can iteratively improve output quality\n",
    "\n",
    "**Next:** Lab 4 - RAG Pipeline Implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
