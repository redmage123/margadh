{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: RAG Pipeline Implementation - SOLUTIONS\n",
    "\n",
    "**Module 4 - Retrieval-Augmented Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "documents = [\n",
    "    \"\"\"Machine Learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. There are three main types: supervised learning, unsupervised learning, and reinforcement learning.\"\"\",\n",
    "    \"\"\"Deep Learning is part of machine learning based on artificial neural networks. Architectures include CNNs, RNNs, and Transformers, applied to computer vision, NLP, and more.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Document Chunking - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(texts: list, chunk_size: int, chunk_overlap: int) -> list:\n",
    "    \"\"\"Create chunks from a list of texts.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    for i, text in enumerate(texts):\n",
    "        chunks = splitter.split_text(text)\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            all_chunks.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"source\": f\"doc_{i}\", \"chunk\": j}\n",
    "            ))\n",
    "    return all_chunks\n",
    "\n",
    "# Test\n",
    "chunks = create_chunks(documents, chunk_size=100, chunk_overlap=20)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "for c in chunks[:3]:\n",
    "    print(f\"  - {c.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Vector Store - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(documents: list) -> FAISS:\n",
    "    chunks = create_chunks(documents, chunk_size=200, chunk_overlap=40)\n",
    "    vector_store = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "    return vector_store\n",
    "\n",
    "def compare_similarities(texts: list):\n",
    "    text_embeddings = [embeddings.embed_query(t) for t in texts]\n",
    "    \n",
    "    print(\"\\nSimilarity Matrix:\")\n",
    "    for i, emb1 in enumerate(text_embeddings):\n",
    "        sims = []\n",
    "        for emb2 in text_embeddings:\n",
    "            sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "            sims.append(f\"{sim:.3f}\")\n",
    "        print(f\"Text {i}: {' | '.join(sims)}\")\n",
    "\n",
    "# Test\n",
    "vector_store = create_vector_store(documents)\n",
    "print(f\"Vector store created with {vector_store.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Semantic Search - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(vector_store, query: str, k: int = 3) -> list:\n",
    "    results = vector_store.similarity_search_with_score(query, k=k)\n",
    "    formatted = []\n",
    "    for doc, score in results:\n",
    "        formatted.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"similarity_score\": 1 - score\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "# Test\n",
    "results = semantic_search(vector_store, \"What is machine learning?\")\n",
    "for r in results:\n",
    "    print(f\"Score: {r['similarity_score']:.3f} - {r['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Complete RAG Pipeline - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, vector_store, llm, k: int = 3):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = llm\n",
    "        self.k = k\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the question.\n",
    "If the context doesn't contain relevant information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "    \n",
    "    def retrieve(self, query: str) -> str:\n",
    "        docs = self.vector_store.similarity_search(query, k=self.k)\n",
    "        return \"\\n\\n\".join([f\"[{i+1}] {d.page_content}\" for i, d in enumerate(docs)])\n",
    "    \n",
    "    def generate(self, question: str, context: str) -> str:\n",
    "        chain = self.prompt | self.llm | StrOutputParser()\n",
    "        return chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    def query(self, question: str) -> dict:\n",
    "        context = self.retrieve(question)\n",
    "        answer = self.generate(question, context)\n",
    "        return {\"question\": question, \"context\": context, \"answer\": answer}\n",
    "\n",
    "# Test\n",
    "rag = RAGPipeline(vector_store, llm)\n",
    "result = rag.query(\"What are the types of machine learning?\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Lab 4 complete! **Next:** Lab 5 - LoRA Fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
