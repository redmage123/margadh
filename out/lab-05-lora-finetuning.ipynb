{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: LoRA Fine-tuning\n",
    "\n",
    "**Module 5 - Model Fine-tuning with Low-Rank Adaptation**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|----------|------------|-----------|----------|\n",
    "| 150 min | Advanced | PEFT + HuggingFace | 4 |\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand LoRA mathematics and implementation\n",
    "- Prepare datasets for instruction fine-tuning\n",
    "- Configure and apply LoRA adapters\n",
    "- Train, evaluate, and merge LoRA weights\n",
    "\n",
    "**Note:** This lab requires a GPU with at least 8GB VRAM. Use Google Colab or a cloud GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets peft accelerate bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Dataset Preparation\n",
    "\n",
    "Prepare a dataset in instruction-following format.\n",
    "\n",
    "**Your Task:** Create and format training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instruction_dataset():\n",
    "    \"\"\"Create a sample instruction-following dataset.\"\"\"\n",
    "    # TODO: Create training data with instruction, input, output fields\n",
    "    training_data = [\n",
    "        {\n",
    "            \"instruction\": \"Explain what machine learning is in simple terms.\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"Machine learning is a type of AI where computers learn patterns from data.\"\n",
    "        },\n",
    "        # Add more examples...\n",
    "    ]\n",
    "    return Dataset.from_list(training_data)\n",
    "\n",
    "\n",
    "def format_instruction(example: dict, tokenizer) -> str:\n",
    "    \"\"\"Format a single example into the Alpaca template.\"\"\"\n",
    "    # TODO: Create the prompt template\n",
    "    # Template: ### Instruction: ... ### Input: (optional) ### Response: ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: LoRA Configuration\n",
    "\n",
    "Set up LoRA configuration and apply it to a base model.\n",
    "\n",
    "**Your Task:** Configure LoRA parameters and analyze trainable params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/phi-2\"  # Small model for learning\n",
    "\n",
    "def create_lora_config(r: int = 8, lora_alpha: int = 16) -> LoraConfig:\n",
    "    \"\"\"Create LoRA configuration.\"\"\"\n",
    "    # TODO: Create LoraConfig with:\n",
    "    # - r (rank)\n",
    "    # - lora_alpha (scaling)\n",
    "    # - target_modules (q_proj, k_proj, v_proj, o_proj)\n",
    "    # - lora_dropout\n",
    "    # - task_type=\"CAUSAL_LM\"\n",
    "    \n",
    "    config = None  # Your code here\n",
    "    return config\n",
    "\n",
    "\n",
    "def analyze_lora_params(model, lora_config):\n",
    "    \"\"\"Analyze trainable parameters with LoRA.\"\"\"\n",
    "    # TODO: Apply LoRA and count trainable vs total params\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Training Setup\n",
    "\n",
    "Configure training with SFTTrainer.\n",
    "\n",
    "**Your Task:** Set up training arguments and trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(model, tokenizer, dataset, output_dir: str = \"./lora_output\"):\n",
    "    \"\"\"Set up LoRA fine-tuning.\"\"\"\n",
    "    # TODO: Create TrainingArguments with:\n",
    "    # - num_train_epochs, batch_size, learning_rate\n",
    "    # - gradient_accumulation_steps\n",
    "    # - fp16=True, optim=\"paged_adamw_8bit\"\n",
    "    \n",
    "    training_args = None  # Your code here\n",
    "    \n",
    "    # TODO: Create SFTTrainer\n",
    "    trainer = None  # Your code here\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Save and Load LoRA Adapters\n",
    "\n",
    "Learn to manage LoRA weights separately from the base model.\n",
    "\n",
    "**Your Task:** Implement save, load, and merge functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def save_lora_adapter(model, output_path: str):\n",
    "    \"\"\"Save only the LoRA adapter weights.\"\"\"\n",
    "    # TODO: Save adapter with model.save_pretrained()\n",
    "    pass\n",
    "\n",
    "\n",
    "def load_lora_adapter(base_model_name: str, adapter_path: str):\n",
    "    \"\"\"Load a LoRA adapter onto a base model.\"\"\"\n",
    "    # TODO: Load base model and apply adapter with PeftModel.from_pretrained()\n",
    "    pass\n",
    "\n",
    "\n",
    "def merge_and_save(model, tokenizer, output_path: str):\n",
    "    \"\"\"Merge LoRA weights into base model and save.\"\"\"\n",
    "    # TODO: Use model.merge_and_unload() then save\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "You've completed Lab 5! Key concepts:\n",
    "\n",
    "- LoRA trains ~0.1% of parameters by injecting low-rank matrices\n",
    "- Adapters can be saved/loaded independently\n",
    "- QLoRA combines quantization with LoRA for memory efficiency\n",
    "\n",
    "**Next:** Lab 6 - Quantization & Optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
