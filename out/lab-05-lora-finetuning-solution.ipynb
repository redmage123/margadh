{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: LoRA Fine-tuning - SOLUTIONS\n",
    "\n",
    "**Module 5 - Model Fine-tuning with Low-Rank Adaptation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "MODEL_NAME = \"microsoft/phi-2\"\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Dataset Preparation - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instruction_dataset():\n",
    "    training_data = [\n",
    "        {\"instruction\": \"Explain machine learning simply.\", \"input\": \"\", \"output\": \"ML is AI that learns from data patterns.\"},\n",
    "        {\"instruction\": \"Summarize this text.\", \"input\": \"Transformers use attention.\", \"output\": \"Attention enables transformers.\"},\n",
    "        {\"instruction\": \"What is LoRA?\", \"input\": \"\", \"output\": \"LoRA trains small adapter matrices instead of full model weights.\"},\n",
    "    ]\n",
    "    return Dataset.from_list(training_data)\n",
    "\n",
    "def format_instruction(example: dict, tokenizer) -> str:\n",
    "    if example[\"input\"]:\n",
    "        return f\"\"\"### Instruction:\\n{example[\"instruction\"]}\\n\\n### Input:\\n{example[\"input\"]}\\n\\n### Response:\\n{example[\"output\"]}{tokenizer.eos_token}\"\"\"\n",
    "    return f\"\"\"### Instruction:\\n{example[\"instruction\"]}\\n\\n### Response:\\n{example[\"output\"]}{tokenizer.eos_token}\"\"\"\n",
    "\n",
    "dataset = create_instruction_dataset()\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: LoRA Configuration - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_config(r: int = 8, lora_alpha: int = 16) -> LoraConfig:\n",
    "    return LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "# Compare configurations\n",
    "for r in [4, 8, 16, 32]:\n",
    "    d, k = 2560, 2560  # Example dimensions\n",
    "    original = d * k\n",
    "    lora = r * (d + k)\n",
    "    print(f\"r={r}: LoRA params = {lora:,} vs Original = {original:,} ({original/lora:.1f}x reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Training Setup - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(model, tokenizer, dataset, output_dir=\"./lora_output\"):\n",
    "    lora_config = create_lora_config(r=8, lora_alpha=16)\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # Format dataset\n",
    "    def format_fn(examples):\n",
    "        texts = [format_instruction({\"instruction\": examples[\"instruction\"][i], \"input\": examples[\"input\"][i], \"output\": examples[\"output\"][i]}, tokenizer) for i in range(len(examples[\"instruction\"]))]\n",
    "        return {\"text\": texts}\n",
    "    \n",
    "    formatted = dataset.map(format_fn, batched=True, remove_columns=dataset.column_names)\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=peft_model,\n",
    "        train_dataset=formatted,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "    )\n",
    "    return trainer, peft_model\n",
    "\n",
    "print(\"Training setup function ready (requires GPU to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Save and Load - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lora_adapter(model, output_path: str):\n",
    "    model.save_pretrained(output_path)\n",
    "    print(f\"Adapter saved to {output_path}\")\n",
    "\n",
    "def load_lora_adapter(base_model_name: str, adapter_path: str):\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\", trust_remote_code=True)\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    return model\n",
    "\n",
    "def merge_and_save(model, tokenizer, output_path: str):\n",
    "    merged = model.merge_and_unload()\n",
    "    merged.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    print(f\"Merged model saved to {output_path}\")\n",
    "\n",
    "print(\"Save/Load functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Lab 5 complete! **Next:** Lab 6 - Quantization & Optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
