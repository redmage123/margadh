{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Transformer Attention Mechanism\n",
    "\n",
    "**Module 1 - Foundations of Modern LLMs**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|----------|------------|-----------|----------|\n",
    "| 90 min | Intermediate | PyTorch | 4 |\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement scaled dot-product attention from scratch\n",
    "- Build multi-head attention mechanism\n",
    "- Visualize attention patterns\n",
    "- Understand masking for causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Scaled Dot-Product Attention\n",
    "\n",
    "Implement the core attention mechanism used in transformers.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- Q (Query): What am I looking for?\n",
    "- K (Key): What do I contain?\n",
    "- V (Value): What information do I provide?\n",
    "- $d_k$: Dimension of keys (scaling factor)\n",
    "\n",
    "**Your Task:** Complete the `scaled_dot_product_attention` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor of shape (batch, seq_len, d_k)\n",
    "        K: Key tensor of shape (batch, seq_len, d_k)\n",
    "        V: Value tensor of shape (batch, seq_len, d_v)\n",
    "        mask: Optional mask tensor\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        attention_weights: Attention weight matrix\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # TODO: Step 1 - Compute attention scores (Q @ K^T)\n",
    "    scores = None  # Your code here\n",
    "    \n",
    "    # TODO: Step 2 - Scale by sqrt(d_k)\n",
    "    scores = None  # Your code here\n",
    "    \n",
    "    # TODO: Step 3 - Apply mask if provided (set masked positions to -inf)\n",
    "    if mask is not None:\n",
    "        pass  # Your code here\n",
    "    \n",
    "    # TODO: Step 4 - Apply softmax to get attention weights\n",
    "    attention_weights = None  # Your code here\n",
    "    \n",
    "    # TODO: Step 5 - Multiply by values\n",
    "    output = None  # Your code here\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: (2, 4, 8)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Expected: (2, 4, 4)\n",
    "print(f\"Weights sum per row: {weights.sum(dim=-1)}\")  # Should be all 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Multi-Head Attention\n",
    "\n",
    "Implement multi-head attention which allows the model to attend to different representation subspaces.\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "Where each head is:\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "**Your Task:** Complete the `MultiHeadAttention` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # TODO: Create linear projections for Q, K, V and output\n",
    "        self.W_q = None  # Your code here\n",
    "        self.W_k = None  # Your code here\n",
    "        self.W_v = None  # Your code here\n",
    "        self.W_o = None  # Your code here\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # TODO: Step 1 - Linear projections\n",
    "        Q = None  # Your code here\n",
    "        K = None  # Your code here\n",
    "        V = None  # Your code here\n",
    "        \n",
    "        # TODO: Step 2 - Reshape for multi-head: (batch, seq, d_model) -> (batch, heads, seq, d_k)\n",
    "        Q = None  # Your code here\n",
    "        K = None  # Your code here\n",
    "        V = None  # Your code here\n",
    "        \n",
    "        # TODO: Step 3 - Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = None, None  # Your code here\n",
    "        \n",
    "        # TODO: Step 4 - Concatenate heads: (batch, heads, seq, d_k) -> (batch, seq, d_model)\n",
    "        attn_output = None  # Your code here\n",
    "        \n",
    "        # TODO: Step 5 - Final linear projection\n",
    "        output = None  # Your code here\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = mha(x, x, x)  # Self-attention\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: (2, 10, 64)\n",
    "print(f\"Attention weights shape: {weights.shape}\")  # Expected: (2, 8, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Attention Visualization\n",
    "\n",
    "Visualize attention patterns to understand what the model is \"looking at\".\n",
    "\n",
    "**Your Task:** Complete the visualization function and analyze the attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, tokens, head_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: Tensor of shape (batch, heads, seq, seq)\n",
    "        tokens: List of token strings\n",
    "        head_idx: Which attention head to visualize\n",
    "    \"\"\"\n",
    "    # TODO: Extract weights for first batch item and specified head\n",
    "    weights = None  # Your code here - shape should be (seq, seq)\n",
    "    \n",
    "    # TODO: Create heatmap using matplotlib\n",
    "    # Hint: Use plt.imshow() with 'Blues' colormap\n",
    "    \n",
    "    # Your visualization code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample attention scenario\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "seq_len = len(tokens)\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Get attention weights\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "_, attention_weights = mha(x, x, x)\n",
    "\n",
    "# Visualize different heads\n",
    "for head in range(min(4, num_heads)):\n",
    "    visualize_attention(attention_weights, tokens, head_idx=head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Causal (Masked) Attention\n",
    "\n",
    "Implement causal masking for autoregressive models like GPT. This prevents the model from \"seeing the future\".\n",
    "\n",
    "**Your Task:** Create a causal mask and apply it to attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal (lower triangular) mask.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of the sequence\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor where True = keep, False = mask\n",
    "    \"\"\"\n",
    "    # TODO: Create lower triangular mask\n",
    "    # Hint: Use torch.tril() or torch.ones() with appropriate masking\n",
    "    mask = None  # Your code here\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test causal mask\n",
    "seq_len = 5\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask:\")\n",
    "print(mask.int())\n",
    "\n",
    "# Expected output:\n",
    "# tensor([[1, 0, 0, 0, 0],\n",
    "#         [1, 1, 0, 0, 0],\n",
    "#         [1, 1, 1, 0, 0],\n",
    "#         [1, 1, 1, 1, 0],\n",
    "#         [1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply causal mask to attention\n",
    "Q = torch.randn(1, seq_len, d_model)\n",
    "K = torch.randn(1, seq_len, d_model)\n",
    "V = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Attention without mask\n",
    "output_no_mask, weights_no_mask = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# Attention with causal mask\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "output_masked, weights_masked = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "print(\"Attention weights WITHOUT mask:\")\n",
    "print(weights_no_mask[0].detach().numpy().round(2))\n",
    "\n",
    "print(\"\\nAttention weights WITH causal mask:\")\n",
    "print(weights_masked[0].detach().numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "Congratulations! You've completed Lab 1. You should now understand:\n",
    "\n",
    "- How scaled dot-product attention computes relevance between tokens\n",
    "- How multi-head attention allows learning multiple attention patterns\n",
    "- How to visualize and interpret attention weights\n",
    "- How causal masking enables autoregressive generation\n",
    "\n",
    "**Next:** Lab 2 - Building LangChain Agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
