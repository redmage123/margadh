{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: Neural Networks from Scratch - SOLUTIONS\n",
        "\n",
        "**Day 2 - Deep Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Activation Functions - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "# Visualize\n",
        "x = np.linspace(-5, 5, 100)\n",
        "plt.figure(figsize=(12, 3))\n",
        "for i, (name, func) in enumerate([('Sigmoid', sigmoid), ('Tanh', tanh), ('ReLU', relu), ('Leaky ReLU', leaky_relu)]):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    plt.plot(x, func(x), linewidth=2)\n",
        "    plt.axhline(y=0, color='k', linewidth=0.5)\n",
        "    plt.axvline(x=0, color='k', linewidth=0.5)\n",
        "    plt.title(name)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Single Neuron - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    def __init__(self, n_inputs, activation='sigmoid'):\n",
        "        self.weights = np.random.randn(n_inputs) * 0.1\n",
        "        self.bias = 0.0\n",
        "        self.activation_name = activation\n",
        "        self.activation = {'sigmoid': sigmoid, 'relu': relu, 'tanh': tanh}[activation]\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        z = np.dot(inputs, self.weights) + self.bias\n",
        "        return self.activation(z)\n",
        "\n",
        "# Test\n",
        "neuron = Neuron(n_inputs=3, activation='sigmoid')\n",
        "inputs = np.array([1.0, 2.0, 3.0])\n",
        "print(f\"Weights: {neuron.weights}\")\n",
        "print(f\"Output: {neuron.forward(inputs):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Dense Layer - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, n_inputs, n_neurons, activation='relu'):\n",
        "        self.weights = np.random.randn(n_inputs, n_neurons) * np.sqrt(2.0 / n_inputs)\n",
        "        self.biases = np.zeros(n_neurons)\n",
        "        self.activation_name = activation\n",
        "        self.activation = {'sigmoid': sigmoid, 'relu': relu, 'tanh': tanh, 'none': lambda x: x}[activation]\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        z = np.dot(inputs, self.weights) + self.biases\n",
        "        return self.activation(z)\n",
        "\n",
        "# Test\n",
        "layer = DenseLayer(n_inputs=4, n_neurons=3, activation='relu')\n",
        "print(f\"Weights shape: {layer.weights.shape}\")\n",
        "print(f\"Single output: {layer.forward(np.array([1.0, 2.0, 3.0, 4.0]))}\")\n",
        "print(f\"Batch output shape: {layer.forward(np.random.randn(5, 4)).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Simple Neural Network - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, activations):\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(DenseLayer(layer_sizes[i], layer_sizes[i+1], activations[i]))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "    \n",
        "    def summary(self):\n",
        "        print(\"Neural Network Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            n = layer.weights.size + layer.biases.size\n",
        "            total += n\n",
        "            print(f\"Layer {i+1}: {layer.weights.shape[0]} -> {layer.weights.shape[1]} ({layer.activation_name})\")\n",
        "        print(f\"Total parameters: {total}\")\n",
        "\n",
        "# Test\n",
        "nn = SimpleNeuralNetwork([4, 8, 4, 1], ['relu', 'relu', 'sigmoid'])\n",
        "nn.summary()\n",
        "print(f\"\\nOutput: {nn.forward(np.random.randn(10, 4)).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Loss Functions - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
        "    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "\n",
        "# Test\n",
        "y_true = np.array([1, 0, 1, 1, 0])\n",
        "y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2])\n",
        "print(f\"MSE Loss: {mse_loss(y_true, y_pred):.4f}\")\n",
        "print(f\"BCE Loss: {binary_cross_entropy(y_true, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Gradient Descent - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent_demo(learning_rate=0.1, n_iterations=50):\n",
        "    f = lambda x: x ** 2\n",
        "    df = lambda x: 2 * x\n",
        "    \n",
        "    x = 4.0\n",
        "    history = [(x, f(x))]\n",
        "    \n",
        "    for i in range(n_iterations):\n",
        "        gradient = df(x)\n",
        "        x = x - learning_rate * gradient\n",
        "        history.append((x, f(x)))\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Visualize\n",
        "history = gradient_descent_demo(learning_rate=0.1, n_iterations=30)\n",
        "xs, ys = zip(*history)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "x_range = np.linspace(-5, 5, 100)\n",
        "plt.plot(x_range, x_range ** 2, 'b-', label='f(x) = x^2')\n",
        "plt.plot(xs, ys, 'ro-', markersize=5, label='Gradient descent')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ys, 'g-o')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final x: {xs[-1]:.6f}, Final loss: {ys[-1]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checkpoint\n",
        "\n",
        "Lab 3 complete! **Next:** Lab 4 - PyTorch Fundamentals"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
