{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Transformer Attention Mechanism - SOLUTIONS\n",
    "\n",
    "**Module 1 - Foundations of Modern LLMs**\n",
    "\n",
    "| Duration | Difficulty | Framework | Exercises |\n",
    "|----------|------------|-----------|----------|\n",
    "| 90 min | Intermediate | PyTorch | 4 |\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement scaled dot-product attention from scratch\n",
    "- Build multi-head attention mechanism\n",
    "- Visualize attention patterns\n",
    "- Understand masking for causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Scaled Dot-Product Attention - SOLUTION\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor of shape (batch, seq_len, d_k) or (batch, heads, seq_len, d_k)\n",
    "        K: Key tensor of shape (batch, seq_len, d_k) or (batch, heads, seq_len, d_k)\n",
    "        V: Value tensor of shape (batch, seq_len, d_v) or (batch, heads, seq_len, d_v)\n",
    "        mask: Optional mask tensor\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        attention_weights: Attention weight matrix\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1 - Compute attention scores (Q @ K^T)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2 - Scale by sqrt(d_k)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3 - Apply mask if provided (set masked positions to -inf)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 4 - Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5 - Multiply by values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: (2, 4, 8)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Expected: (2, 4, 4)\n",
    "print(f\"Weights sum per row: {weights.sum(dim=-1)}\")  # Should be all 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Multi-Head Attention - SOLUTION\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V and output\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        seq_len = Q.size(1)\n",
    "        \n",
    "        # Step 1 - Linear projections\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "        \n",
    "        # Step 2 - Reshape for multi-head: (batch, seq, d_model) -> (batch, heads, seq, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Step 3 - Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Step 4 - Concatenate heads: (batch, heads, seq, d_k) -> (batch, seq, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Step 5 - Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = mha(x, x, x)  # Self-attention\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: (2, 10, 64)\n",
    "print(f\"Attention weights shape: {weights.shape}\")  # Expected: (2, 8, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Attention Visualization - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, tokens, head_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: Tensor of shape (batch, heads, seq, seq)\n",
    "        tokens: List of token strings\n",
    "        head_idx: Which attention head to visualize\n",
    "    \"\"\"\n",
    "    # Extract weights for first batch item and specified head\n",
    "    weights = attention_weights[0, head_idx].detach().numpy()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(weights, cmap='Blues', aspect='auto')\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Key (attending to)')\n",
    "    ax.set_ylabel('Query (attending from)')\n",
    "    ax.set_title(f'Attention Weights - Head {head_idx}')\n",
    "    \n",
    "    # Add value annotations\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            text = ax.text(j, i, f'{weights[i, j]:.2f}',\n",
    "                          ha='center', va='center', fontsize=8,\n",
    "                          color='white' if weights[i, j] > 0.5 else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample attention scenario\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "seq_len = len(tokens)\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Get attention weights\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "_, attention_weights = mha(x, x, x)\n",
    "\n",
    "# Visualize different heads\n",
    "for head in range(min(4, num_heads)):\n",
    "    visualize_attention(attention_weights, tokens, head_idx=head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Causal (Masked) Attention - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal (lower triangular) mask.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of the sequence\n",
    "    \n",
    "    Returns:\n",
    "        mask: Boolean tensor where True = keep, False = mask\n",
    "    \"\"\"\n",
    "    # Create lower triangular mask\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test causal mask\n",
    "seq_len = 5\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask:\")\n",
    "print(mask.int())\n",
    "\n",
    "# Expected output:\n",
    "# tensor([[1, 0, 0, 0, 0],\n",
    "#         [1, 1, 0, 0, 0],\n",
    "#         [1, 1, 1, 0, 0],\n",
    "#         [1, 1, 1, 1, 0],\n",
    "#         [1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply causal mask to attention\n",
    "Q = torch.randn(1, seq_len, d_model)\n",
    "K = torch.randn(1, seq_len, d_model)\n",
    "V = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# For scaled_dot_product_attention, we need to handle the shape\n",
    "# Project to d_k for the test\n",
    "d_k_test = 8\n",
    "Q_test = torch.randn(1, seq_len, d_k_test)\n",
    "K_test = torch.randn(1, seq_len, d_k_test)\n",
    "V_test = torch.randn(1, seq_len, d_k_test)\n",
    "\n",
    "# Attention without mask\n",
    "output_no_mask, weights_no_mask = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "# Attention with causal mask\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "output_masked, weights_masked = scaled_dot_product_attention(Q_test, K_test, V_test, mask=causal_mask)\n",
    "\n",
    "print(\"Attention weights WITHOUT mask:\")\n",
    "print(weights_no_mask[0].detach().numpy().round(2))\n",
    "\n",
    "print(\"\\nAttention weights WITH causal mask:\")\n",
    "print(weights_masked[0].detach().numpy().round(2))\n",
    "\n",
    "print(\"\\nNotice: With causal mask, upper triangular values are 0 (future tokens masked)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the causal mask effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without mask\n",
    "im1 = axes[0].imshow(weights_no_mask[0].detach().numpy(), cmap='Blues')\n",
    "axes[0].set_title('Attention WITHOUT Causal Mask')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# With mask\n",
    "im2 = axes[1].imshow(weights_masked[0].detach().numpy(), cmap='Blues')\n",
    "axes[1].set_title('Attention WITH Causal Mask')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Complete Transformer Block\n",
    "\n",
    "Here's how multi-head attention fits into a complete transformer block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A complete transformer block with attention + FFN.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, attn_weights = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # FFN with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "# Test transformer block\n",
    "block = TransformerBlock(d_model=64, num_heads=8, d_ff=256)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output, weights = block(x)\n",
    "print(f\"Transformer block output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "Congratulations! You've completed Lab 1. You should now understand:\n",
    "\n",
    "- How scaled dot-product attention computes relevance between tokens\n",
    "- How multi-head attention allows learning multiple attention patterns\n",
    "- How to visualize and interpret attention weights\n",
    "- How causal masking enables autoregressive generation\n",
    "\n",
    "**Next:** Lab 2 - Building LangChain Agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
