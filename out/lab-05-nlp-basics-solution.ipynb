{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: NLP Basics - SOLUTIONS\n",
        "\n",
        "**Day 3 - From Deep Learning to LLMs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Text Preprocessing - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize_simple(text):\n",
        "    return text.split()\n",
        "\n",
        "def build_vocabulary(texts, min_freq=1):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        tokens = tokenize_simple(clean_text(text))\n",
        "        word_counts.update(tokens)\n",
        "    \n",
        "    vocab = [word for word, count in word_counts.items() if count >= min_freq]\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    for word in vocab:\n",
        "        word2idx[word] = len(word2idx)\n",
        "    \n",
        "    idx2word = {v: k for k, v in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "# Test\n",
        "sample_texts = [\n",
        "    \"Hello, World! This is a TEST.\",\n",
        "    \"Machine Learning is AMAZING!!!\",\n",
        "    \"Neural networks learn from data.\"\n",
        "]\n",
        "print(f\"Cleaned: {clean_text(sample_texts[0])}\")\n",
        "word2idx, idx2word = build_vocabulary(sample_texts)\n",
        "print(f\"Vocabulary size: {len(word2idx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Text to Sequences - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_text(text, word2idx, max_length=None):\n",
        "    tokens = tokenize_simple(clean_text(text))\n",
        "    indices = [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
        "    \n",
        "    if max_length:\n",
        "        if len(indices) < max_length:\n",
        "            indices = indices + [0] * (max_length - len(indices))\n",
        "        else:\n",
        "            indices = indices[:max_length]\n",
        "    \n",
        "    return indices\n",
        "\n",
        "def decode_sequence(indices, idx2word):\n",
        "    words = [idx2word[idx] for idx in indices if idx != 0]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Test\n",
        "test_text = \"Neural networks are powerful models.\"\n",
        "encoded = encode_text(test_text, word2idx, max_length=10)\n",
        "print(f\"Encoded: {encoded}\")\n",
        "print(f\"Decoded: {decode_sequence(encoded, idx2word)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Word Embeddings - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleEmbedding:\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.1\n",
        "    \n",
        "    def __call__(self, indices):\n",
        "        return self.embeddings[indices]\n",
        "\n",
        "def compute_cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "# Test\n",
        "simple_emb = SimpleEmbedding(len(word2idx), 50)\n",
        "print(f\"Embedding shape: {simple_emb.embeddings.shape}\")\n",
        "print(f\"Lookup shape: {simple_emb([2, 3, 4]).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Self-Attention - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    d_k = query.shape[-1]\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "    scores = scores / np.sqrt(d_k)\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    return output, attention_weights\n",
        "\n",
        "def visualize_attention(attention_weights, tokens):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(attention_weights, cmap='Blues')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
        "    plt.yticks(range(len(tokens)), tokens)\n",
        "    plt.xlabel('Keys')\n",
        "    plt.ylabel('Queries')\n",
        "    plt.title('Self-Attention Weights')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test\n",
        "embeddings = torch.randn(5, 8)\n",
        "output, weights = scaled_dot_product_attention(embeddings, embeddings, embeddings)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention weights:\\n{weights.numpy().round(3)}\")\n",
        "\n",
        "tokens = ['The', 'cat', 'sat', 'on', 'mat']\n",
        "visualize_attention(weights.numpy(), tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Transformer Block - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.scale = np.sqrt(embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "        \n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        return torch.matmul(weights, V)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super().__init__()\n",
        "        self.attention = SelfAttention(embed_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        attn_out = self.attention(x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "        return x\n",
        "\n",
        "# Test\n",
        "transformer = TransformerBlock(64, 128)\n",
        "test_input = torch.randn(2, 10, 64)\n",
        "output = transformer(test_input)\n",
        "print(f\"Input: {test_input.shape}, Output: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Text Classifier - SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.attention = SelfAttention(embed_dim)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # (batch, seq, embed)\n",
        "        attended = self.attention(embedded)  # (batch, seq, embed)\n",
        "        pooled = attended.mean(dim=1)  # (batch, embed)\n",
        "        return self.classifier(pooled)  # (batch, classes)\n",
        "\n",
        "# Test\n",
        "classifier = TextClassifier(1000, 32, 2)\n",
        "batch = torch.randint(0, 1000, (4, 20))\n",
        "output = classifier(batch)\n",
        "print(f\"Input: {batch.shape}, Output: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checkpoint\n",
        "\n",
        "Lab 5 complete! **Next:** Lab 6 - LLM APIs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
