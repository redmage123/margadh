{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: Neural Networks from Scratch\n",
        "\n",
        "**Day 2 - Deep Learning**\n",
        "\n",
        "| Duration | Difficulty | Prerequisites |\n",
        "|----------|------------|---------------|\n",
        "| 90 min | Intermediate | Labs 1-2 |\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand neuron and layer structure\n",
        "- Implement activation functions\n",
        "- Build forward propagation\n",
        "- Understand gradient descent basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 1: Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity, allowing neural networks to learn complex patterns.\n",
        "\n",
        "**Your Task:** Implement common activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid activation: f(x) = 1 / (1 + e^(-x))\n",
        "    Output range: (0, 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement sigmoid\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    \"\"\"\n",
        "    Tanh activation: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
        "    Output range: (-1, 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement tanh (hint: use np.tanh)\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    \"\"\"\n",
        "    ReLU activation: f(x) = max(0, x)\n",
        "    Output range: [0, inf)\n",
        "    \"\"\"\n",
        "    # TODO: Implement ReLU\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"\n",
        "    Leaky ReLU: f(x) = x if x > 0, else alpha * x\n",
        "    \"\"\"\n",
        "    # TODO: Implement Leaky ReLU\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize activation functions\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "activations = [('Sigmoid', sigmoid), ('Tanh', tanh), ('ReLU', relu), ('Leaky ReLU', leaky_relu)]\n",
        "\n",
        "for i, (name, func) in enumerate(activations):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    if func(x) is not None:\n",
        "        plt.plot(x, func(x), linewidth=2)\n",
        "        plt.axhline(y=0, color='k', linewidth=0.5)\n",
        "        plt.axvline(x=0, color='k', linewidth=0.5)\n",
        "    plt.title(name)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('f(x)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 2: Single Neuron\n",
        "\n",
        "A neuron computes: output = activation(sum(inputs * weights) + bias)\n",
        "\n",
        "**Your Task:** Implement a single neuron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    def __init__(self, n_inputs, activation='sigmoid'):\n",
        "        \"\"\"\n",
        "        Initialize a neuron with random weights and bias.\n",
        "        \n",
        "        Args:\n",
        "            n_inputs: Number of input features\n",
        "            activation: 'sigmoid', 'relu', or 'tanh'\n",
        "        \"\"\"\n",
        "        # TODO: Initialize weights randomly (small values)\n",
        "        # Hint: np.random.randn(n_inputs) * 0.1\n",
        "        self.weights = None\n",
        "        \n",
        "        # TODO: Initialize bias to 0\n",
        "        self.bias = None\n",
        "        \n",
        "        # Store activation function\n",
        "        self.activation_name = activation\n",
        "        self.activation = {'sigmoid': sigmoid, 'relu': relu, 'tanh': tanh}[activation]\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Compute the output of the neuron.\n",
        "        \n",
        "        1. Calculate weighted sum: z = sum(inputs * weights) + bias\n",
        "        2. Apply activation: output = activation(z)\n",
        "        \"\"\"\n",
        "        # TODO: Calculate weighted sum (z)\n",
        "        # Hint: np.dot(inputs, self.weights) + self.bias\n",
        "        z = None\n",
        "        \n",
        "        # TODO: Apply activation function\n",
        "        output = None\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 2\n",
        "neuron = Neuron(n_inputs=3, activation='sigmoid')\n",
        "inputs = np.array([1.0, 2.0, 3.0])\n",
        "\n",
        "if neuron.weights is not None:\n",
        "    print(f\"Weights: {neuron.weights}\")\n",
        "    print(f\"Bias: {neuron.bias}\")\n",
        "    output = neuron.forward(inputs)\n",
        "    print(f\"Output: {output}\")\n",
        "else:\n",
        "    print(\"Implement the Neuron class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 3: Dense Layer\n",
        "\n",
        "A dense (fully connected) layer contains multiple neurons.\n",
        "\n",
        "**Your Task:** Implement a dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, n_inputs, n_neurons, activation='relu'):\n",
        "        \"\"\"\n",
        "        Initialize a dense layer.\n",
        "        \n",
        "        Args:\n",
        "            n_inputs: Number of input features\n",
        "            n_neurons: Number of neurons in this layer\n",
        "            activation: Activation function name\n",
        "        \"\"\"\n",
        "        # TODO: Initialize weight matrix (n_inputs x n_neurons)\n",
        "        # Use Xavier initialization: * np.sqrt(2.0 / n_inputs)\n",
        "        self.weights = None\n",
        "        \n",
        "        # TODO: Initialize bias vector (n_neurons,)\n",
        "        self.biases = None\n",
        "        \n",
        "        self.activation_name = activation\n",
        "        self.activation = {'sigmoid': sigmoid, 'relu': relu, 'tanh': tanh, 'none': lambda x: x}[activation]\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through the layer.\n",
        "        \n",
        "        inputs shape: (batch_size, n_inputs) or (n_inputs,)\n",
        "        output shape: (batch_size, n_neurons) or (n_neurons,)\n",
        "        \"\"\"\n",
        "        # TODO: Calculate z = inputs @ weights + biases\n",
        "        z = None\n",
        "        \n",
        "        # TODO: Apply activation\n",
        "        output = None\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 3\n",
        "layer = DenseLayer(n_inputs=4, n_neurons=3, activation='relu')\n",
        "\n",
        "if layer.weights is not None:\n",
        "    print(f\"Weights shape: {layer.weights.shape}\")\n",
        "    print(f\"Biases shape: {layer.biases.shape}\")\n",
        "    \n",
        "    # Single sample\n",
        "    single_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
        "    output = layer.forward(single_input)\n",
        "    print(f\"Single input output: {output}\")\n",
        "    \n",
        "    # Batch of samples\n",
        "    batch_input = np.random.randn(5, 4)\n",
        "    batch_output = layer.forward(batch_input)\n",
        "    print(f\"Batch output shape: {batch_output.shape}\")\n",
        "else:\n",
        "    print(\"Implement the DenseLayer class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 4: Simple Neural Network\n",
        "\n",
        "**Your Task:** Stack layers to create a neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, activations):\n",
        "        \"\"\"\n",
        "        Create a neural network with given architecture.\n",
        "        \n",
        "        Args:\n",
        "            layer_sizes: List of layer sizes [input, hidden1, hidden2, ..., output]\n",
        "            activations: List of activation functions for each layer (except input)\n",
        "        \n",
        "        Example:\n",
        "            layer_sizes = [4, 8, 4, 1]  # 4 inputs, 2 hidden layers, 1 output\n",
        "            activations = ['relu', 'relu', 'sigmoid']\n",
        "        \"\"\"\n",
        "        self.layers = []\n",
        "        \n",
        "        # TODO: Create layers\n",
        "        # For i in range(len(layer_sizes) - 1):\n",
        "        #   Create DenseLayer(layer_sizes[i], layer_sizes[i+1], activations[i])\n",
        "        #   Append to self.layers\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through all layers.\n",
        "        \"\"\"\n",
        "        # TODO: Pass input through each layer sequentially\n",
        "        pass\n",
        "    \n",
        "    def summary(self):\n",
        "        \"\"\"Print network architecture.\"\"\"\n",
        "        print(\"Neural Network Summary\")\n",
        "        print(\"=\" * 50)\n",
        "        total_params = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            n_params = layer.weights.size + layer.biases.size\n",
        "            total_params += n_params\n",
        "            print(f\"Layer {i+1}: {layer.weights.shape[0]} -> {layer.weights.shape[1]} ({layer.activation_name})\")\n",
        "            print(f\"         Parameters: {n_params}\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 4\n",
        "nn = SimpleNeuralNetwork(\n",
        "    layer_sizes=[4, 8, 4, 1],\n",
        "    activations=['relu', 'relu', 'sigmoid']\n",
        ")\n",
        "\n",
        "if len(nn.layers) > 0:\n",
        "    nn.summary()\n",
        "    \n",
        "    # Test forward pass\n",
        "    test_input = np.random.randn(10, 4)\n",
        "    output = nn.forward(test_input)\n",
        "    print(f\"\\nInput shape: {test_input.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Output range: [{output.min():.4f}, {output.max():.4f}]\")\n",
        "else:\n",
        "    print(\"Implement SimpleNeuralNetwork\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 5: Loss Functions\n",
        "\n",
        "Loss functions measure how wrong our predictions are.\n",
        "\n",
        "**Your Task:** Implement common loss functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Mean Squared Error for regression.\n",
        "    \n",
        "    MSE = mean((y_true - y_pred)^2)\n",
        "    \"\"\"\n",
        "    # TODO: Implement MSE loss\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
        "    \"\"\"\n",
        "    Binary Cross-Entropy for classification.\n",
        "    \n",
        "    BCE = -mean(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))\n",
        "    \n",
        "    Note: Clip y_pred to avoid log(0)\n",
        "    \"\"\"\n",
        "    # TODO: Clip predictions to [epsilon, 1-epsilon]\n",
        "    y_pred_clipped = None\n",
        "    \n",
        "    # TODO: Calculate BCE\n",
        "    loss = None\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Exercise 5\n",
        "y_true = np.array([1, 0, 1, 1, 0])\n",
        "y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2])\n",
        "\n",
        "mse = mse_loss(y_true, y_pred)\n",
        "bce = binary_cross_entropy(y_true, y_pred)\n",
        "\n",
        "print(f\"MSE Loss: {mse}\")\n",
        "print(f\"BCE Loss: {bce}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Exercise 6: Gradient Descent Visualization\n",
        "\n",
        "**Your Task:** Understand how gradient descent finds the minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent_demo(learning_rate=0.1, n_iterations=50):\n",
        "    \"\"\"\n",
        "    Demonstrate gradient descent on f(x) = x^2.\n",
        "    \n",
        "    Derivative: df/dx = 2x\n",
        "    Update rule: x_new = x - learning_rate * gradient\n",
        "    \"\"\"\n",
        "    # Function and its derivative\n",
        "    f = lambda x: x ** 2\n",
        "    df = lambda x: 2 * x\n",
        "    \n",
        "    # Starting point\n",
        "    x = 4.0\n",
        "    history = [(x, f(x))]\n",
        "    \n",
        "    # TODO: Implement gradient descent loop\n",
        "    for i in range(n_iterations):\n",
        "        # TODO: Calculate gradient\n",
        "        gradient = None\n",
        "        \n",
        "        # TODO: Update x\n",
        "        # x = x - learning_rate * gradient\n",
        "        \n",
        "        # Record history\n",
        "        if gradient is not None:\n",
        "            history.append((x, f(x)))\n",
        "    \n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test and visualize\n",
        "history = gradient_descent_demo(learning_rate=0.1, n_iterations=30)\n",
        "\n",
        "if len(history) > 1:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    # Plot 1: Function with descent path\n",
        "    plt.subplot(1, 2, 1)\n",
        "    x_range = np.linspace(-5, 5, 100)\n",
        "    plt.plot(x_range, x_range ** 2, 'b-', label='f(x) = x^2')\n",
        "    \n",
        "    xs, ys = zip(*history)\n",
        "    plt.plot(xs, ys, 'ro-', markersize=5, label='Gradient descent')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('f(x)')\n",
        "    plt.title('Gradient Descent Path')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Loss over iterations\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(ys, 'g-o')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Over Time')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Final x: {xs[-1]:.6f}\")\n",
        "    print(f\"Final loss: {ys[-1]:.6f}\")\n",
        "else:\n",
        "    print(\"Implement gradient_descent_demo()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Checkpoint\n",
        "\n",
        "Congratulations! You've completed Lab 3.\n",
        "\n",
        "### Key Takeaways:\n",
        "- Neurons compute weighted sums + activation\n",
        "- Activation functions add non-linearity\n",
        "- Layers stack neurons; networks stack layers\n",
        "- Loss functions measure prediction error\n",
        "- Gradient descent minimizes loss\n",
        "\n",
        "**Next:** Lab 4 - PyTorch Fundamentals"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
